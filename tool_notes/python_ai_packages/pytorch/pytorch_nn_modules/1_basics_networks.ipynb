{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ae6117d",
   "metadata": {},
   "source": [
    "1. `nn.linear`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a53402",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Do linear at the last dimension, no matter how many dimensions at prev\n",
    "\"\"\"\n",
    "\n",
    "# eg\n",
    "m = nn.Linear(20, 30)\n",
    "input = torch.randn(128, 20)    \n",
    "output = m(input)   # [128, 30]\n",
    "\n",
    "m = nn.Linear(20, 30)\n",
    "input = torch.randn(64, 128, 20)    \n",
    "output = m(input)   # [64, 128, 30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8e9518",
   "metadata": {},
   "source": [
    "2. residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39cc3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "template\n",
    "\"\"\"\n",
    "class ResidualBlock(nn.Module):\n",
    "    # Follows \"Identity Mappings in Deep Residual Networks\", uses LayerNorm instead of BatchNorm, and LeakyReLU instead of ReLU\n",
    "    def __init__(self, feat_in=128, feat_out=128, feat_hidden=256, drop_out=0.0, use_norm=True):\n",
    "        super().__init__()\n",
    "        # Define the residual block with or without normalization\n",
    "        if use_norm:\n",
    "            self.block = nn.Sequential(\n",
    "                nn.LayerNorm(feat_in),  # Layer normalization on input features\n",
    "                nn.LeakyReLU(negative_slope=0.1),  # LeakyReLU activation\n",
    "                nn.Dropout(p=drop_out),\n",
    "                nn.Linear(feat_in, feat_hidden),  # Linear layer transforming input to hidden features\n",
    "                nn.LayerNorm(feat_hidden),  # Layer normalization on hidden features\n",
    "                nn.LeakyReLU(negative_slope=0.1),  # LeakyReLU activation\n",
    "                nn.Dropout(p=drop_out),\n",
    "                nn.Linear(feat_hidden, feat_out)  # Linear layer transforming hidden to output features\n",
    "            )\n",
    "        else:\n",
    "            self.block = nn.Sequential(\n",
    "                nn.LeakyReLU(negative_slope=0.1),  # LeakyReLU activation\n",
    "                nn.Dropout(p=drop_out),\n",
    "                nn.Linear(feat_in, feat_hidden),  # Linear layer transforming input to hidden features\n",
    "                nn.LeakyReLU(negative_slope=0.1),  # LeakyReLU activation\n",
    "                nn.Dropout(p=drop_out),\n",
    "                nn.Linear(feat_hidden, feat_out)  # Linear layer transforming hidden to output features\n",
    "            )\n",
    "\n",
    "        # Define the bypass connection\n",
    "        if feat_in != feat_out:\n",
    "            self.bypass = nn.Linear(feat_in, feat_out)  # Linear layer to match dimensions if they differ\n",
    "        else:\n",
    "            self.bypass = nn.Identity()  # Identity layer if input and output dimensions are the same\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        # Forward pass: apply the block and add the bypass connection\n",
    "        return self.block(input_data) + self.bypass(input_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
